{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EuWvUi1b7T7_"
   },
   "source": [
    "# Final Project Code Part 4 Improved AE On CIFAR10\n",
    "\n",
    "**Name: Yiqun Hao**\n",
    "\n",
    "**Content**: In this notebook:\n",
    "\n",
    "\n",
    "1. I did experiments on CIFAR10 using the code from part 3: the network with \"push\" operation having AE removed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QvqANx2A_b7s"
   },
   "source": [
    "#### Import all packages used in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "qBoJnKWp6Iff"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EZRA\\anaconda3\\envs\\pytorch\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Main package: PyTorch\n",
    "import torch\n",
    "# TorchVision contains image datasets\n",
    "import torchvision\n",
    "# Our networks\n",
    "import torch.nn as nn\n",
    "# Our activation\n",
    "import torch.nn.functional as F\n",
    "# Optimizer\n",
    "import torch.optim as optim\n",
    "\n",
    "# Draw graphs to visualize\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0. Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CAE Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lambda's are the ratios between the four error terms\n",
    "# lambda_class = 20\n",
    "lambda_class = 10\n",
    "lambda_1 = 1              # 1 and 2 here corresponds to the notation we used in the paper\n",
    "lambda_2 = 1\n",
    "\n",
    "# the number of prototypes\n",
    "n_prototypes = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PgrKXQ6M_UQ5"
   },
   "source": [
    "#### 1. We first import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1b88994fc30>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_epochs = 150\n",
    "batch_size = 10\n",
    "learning_rate = 0.0005\n",
    "momentum = 0.5\n",
    "random_seed = 1\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# 1. We first construct a transform to normalize image data\n",
    "transform = torchvision.transforms.Compose([\n",
    "    # To [0,1]\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    # To [-1,1] first () is the mean of RGB second () is the std of RBG\n",
    "    torchvision.transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
    "])\n",
    "# 2. We import data\n",
    "train_set = torchvision.datasets.STL10(root='./data',split='train',download=True,transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "test_set = torchvision.datasets.STL10(root='./data',split='test',download=True,transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_iterations_train 500\n",
      "num_iterations_test 800\n"
     ]
    }
   ],
   "source": [
    "# num_iterations_train = 0\n",
    "# num_iterations_test = 0\n",
    "# for i, (inputs, labels) in enumerate(train_loader):\n",
    "#     num_iterations_train+=1\n",
    "    \n",
    "    \n",
    "# for i, (inputs, labels) in enumerate(test_loader):\n",
    "#     num_iterations_test +=1\n",
    "    \n",
    "# print(\"num_iterations_train\", num_iterations_train)\n",
    "# print(\"num_iterations_test\", num_iterations_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0besyr_yBh4V"
   },
   "source": [
    "#### 2. We construct our network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu1): ReLU()\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu2): ReLU()\n",
      "  (conv5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv7): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1))\n",
      "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu3): ReLU()\n",
      "  (conv8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv9): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv10): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1))\n",
      "  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (bn4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu4): ReLU()\n",
      "  (conv11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv12): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv13): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1))\n",
      "  (pool5): MaxPool2d(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (bn5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu5): ReLU()\n",
      "  (fc14): Linear(in_features=18432, out_features=1024, bias=True)\n",
      "  (drop1): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc15): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (drop2): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc16): Linear(in_features=1024, out_features=40, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 64, 3, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(128, 128, 3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        self.conv5 = nn.Conv2d(128, 128, 3, padding=1)\n",
    "        self.conv6 = nn.Conv2d(128, 128, 3, padding=1)\n",
    "        self.conv7 = nn.Conv2d(128, 128, 1, padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "        self.conv8 = nn.Conv2d(128, 256, 3, padding=1)\n",
    "        self.conv9 = nn.Conv2d(256, 256, 3, padding=1)\n",
    "        self.conv10 = nn.Conv2d(256, 256, 1, padding=1)\n",
    "        self.pool4 = nn.MaxPool2d(2, 2, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        self.relu4 = nn.ReLU()\n",
    "\n",
    "        self.conv11 = nn.Conv2d(256, 512, 3, padding=1)\n",
    "        self.conv12 = nn.Conv2d(512, 512, 3, padding=1)\n",
    "        self.conv13 = nn.Conv2d(512, 512, 1, padding=1)\n",
    "        self.pool5 = nn.MaxPool2d(2, 2, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(512)\n",
    "        self.relu5 = nn.ReLU()\n",
    "\n",
    "        self.fc14 = nn.Linear(512 * 6 * 6, 1024)\n",
    "        self.drop1 = nn.Dropout2d()\n",
    "        self.fc15 = nn.Linear(1024, 1024)\n",
    "        self.drop2 = nn.Dropout2d()\n",
    "        self.fc16 = nn.Linear(1024, 40)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "\n",
    "        x = self.conv5(x)\n",
    "        x = self.conv6(x)\n",
    "        x = self.conv7(x)\n",
    "        x = self.pool3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu3(x)\n",
    "\n",
    "        x = self.conv8(x)\n",
    "        x = self.conv9(x)\n",
    "        x = self.conv10(x)\n",
    "        x = self.pool4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.relu4(x)\n",
    "\n",
    "        x = self.conv11(x)\n",
    "        x = self.conv12(x)\n",
    "        x = self.conv13(x)\n",
    "        x = self.pool5(x)\n",
    "        x = self.bn5(x)\n",
    "        x = self.relu5(x)\n",
    "        x = x.view(-1, 512 * 6 * 6)\n",
    "        x = F.relu(self.fc14(x))\n",
    "        x = self.drop1(x)\n",
    "        x = F.relu(self.fc15(x))\n",
    "        x = self.drop2(x)\n",
    "        x = self.fc16(x)\n",
    "        \n",
    "        return x\n",
    "print(Encoder())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Encoder(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Encoder, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(3, 6, 5, stride = 2)\n",
    "#         self.conv2 = nn.Conv2d(6, 16, 5, stride = 2)\n",
    "#         self.fc1 = nn.Linear(400, 40)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # Input: 3 * 32 * 32 = 3072\n",
    "#         # 1. 6 * 14 * 14 = 1176  or 6*15*15\n",
    "#         x = F.relu(self.conv1(x))\n",
    "\n",
    "#         # 3. 16 * 5 * 5 = 400\n",
    "#         x = F.relu(self.conv2(x))\n",
    "\n",
    "#         # Flatten layer\n",
    "#         x = x.view(-1, 400)\n",
    "\n",
    "#         # Enter into classification\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         return x\n",
    "    \n",
    "#         return x\n",
    "# # print(Encoder())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Prototype Classification Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions used to calculate the distance between encoded_input and prototype vectors\n",
    "def list_of_distances(X, Y):\n",
    "    '''\n",
    "    Given a list of vectors, X = [x_1, ..., x_n], and another list of vectors,\n",
    "    Y = [y_1, ... , y_m], we return a list of vectors\n",
    "            [[d(x_1, y_1), d(x_1, y_2), ... , d(x_1, y_m)],\n",
    "             ...\n",
    "             [d(x_n, y_1), d(x_n, y_2), ... , d(x_n, y_m)]],\n",
    "    where the distance metric used is the sqared euclidean distance.\n",
    "    The computation is achieved through a clever use of broadcasting.\n",
    "    '''\n",
    "    XX = list_of_norms(X).view(-1, 1)\n",
    "    YY = list_of_norms(Y).view(1, -1)\n",
    "    output = XX + YY - 2 * torch.matmul(X, Y.t())\n",
    "\n",
    "    return output\n",
    "\n",
    "def list_of_norms(X):\n",
    "    '''\n",
    "    X is a list of vectors X = [x_1, ..., x_n], we return\n",
    "        [d(x_1, x_1), d(x_2, x_2), ... , d(x_n, x_n)], where the distance\n",
    "    function is the squared euclidean distance.\n",
    "    '''\n",
    "    return torch.sum(torch.pow(X,2), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCN()\n"
     ]
    }
   ],
   "source": [
    "class PCN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PCN, self).__init__()\n",
    "        self.prototype_feature_vectors = nn.Parameter(torch.rand(n_prototypes, 40))\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "# x.shape:  torch.Size([100, 10, 2, 2])\n",
    "# feature_vectors.shape:  torch.Size([100, 40])\n",
    "# prototype_feature_vectors.shape:  torch.Size([10, 40])\n",
    "# feature_vector_distances.shape is the list of distances from each prototype to every x_i in the latent space:  torch.Size([10, 100])\n",
    "# prototype_distances.shape is the list of distances from each x_i to every prototype in the latent space :  torch.Size([100, 10])\n",
    "    def forward(self, feature_vectors):\n",
    "        \n",
    "        # ----------------------------------------------------------------------------------------------------------\n",
    "        # Prototype layer\n",
    "        prototype_distances = list_of_distances(feature_vectors, self.prototype_feature_vectors)\n",
    "        feature_vector_distances = list_of_distances(self.prototype_feature_vectors, feature_vectors)\n",
    "\n",
    "\n",
    "\n",
    "        # ----------------------------------------------------------------------------------------------------------\n",
    "\n",
    "        # Fully-connected layer\n",
    "        output = prototype_distances * -1\n",
    "        return output, prototype_distances, feature_vector_distances, self.prototype_feature_vectors\n",
    "\n",
    "\n",
    "    \n",
    "print(PCN())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LastLayer(\n",
      "  (fl): Linear(in_features=15, out_features=10, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class LastLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LastLayer, self).__init__()\n",
    "        self.fl = nn.Linear(n_prototypes, 10, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fl(x)\n",
    "        return x\n",
    "    def showWeight(self):\n",
    "        print(self.fl.weight) \n",
    "print(LastLayer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overall Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyModel(\n",
      "  (encoder): Encoder(\n",
      "    (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu1): ReLU()\n",
      "    (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (conv4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (pool2): MaxPool2d(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu2): ReLU()\n",
      "    (conv5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (conv6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (conv7): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1))\n",
      "    (pool3): MaxPool2d(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu3): ReLU()\n",
      "    (conv8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (conv9): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (conv10): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1))\n",
      "    (pool4): MaxPool2d(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (bn4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu4): ReLU()\n",
      "    (conv11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (conv12): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (conv13): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1))\n",
      "    (pool5): MaxPool2d(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (bn5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu5): ReLU()\n",
      "    (fc14): Linear(in_features=18432, out_features=1024, bias=True)\n",
      "    (drop1): Dropout2d(p=0.5, inplace=False)\n",
      "    (fc15): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (drop2): Dropout2d(p=0.5, inplace=False)\n",
      "    (fc16): Linear(in_features=1024, out_features=40, bias=True)\n",
      "  )\n",
      "  (pcn): PCN()\n",
      "  (lastLayer): LastLayer(\n",
      "    (fl): Linear(in_features=15, out_features=10, bias=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.pcn = PCN()\n",
    "        self.lastLayer = LastLayer()\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded_input = self.encoder(x)\n",
    "        output, prototype_distances, feature_vector_distances, prototype_vectors = self.pcn(encoded_input)\n",
    "        output = self.lastLayer(output)\n",
    "        return encoded_input, output, prototype_distances, feature_vector_distances, prototype_vectors\n",
    "    \n",
    "    def showWeight(self):\n",
    "        self.lastLayer.showWeight()\n",
    "    \n",
    "    \n",
    "print(MyModel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "the error function consists of 4 terms, the autoencoder loss,\n",
    "the classification loss, and the two requirements that every feature vector in\n",
    "X look like at least one of the prototype feature vectors and every prototype\n",
    "feature vector look like at least one of the feature vectors in X.\n",
    "'''\n",
    "class LossFunc(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LossFunc, self).__init__()\n",
    "        self.crossLoss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, outputs, labels, images, feature_vector_distances, prototype_distances):\n",
    "        class_error = self.crossLoss(outputs,labels)\n",
    "        error_2 = torch.mean(torch.min(prototype_distances, axis = 1).values)\n",
    "        error_1 = torch.mean(torch.min(feature_vector_distances, axis = 1).values)\n",
    "        total_error = lambda_class * class_error +\\\n",
    "                      lambda_1 * error_1 +\\\n",
    "                      lambda_2 * error_2\n",
    "        return class_error, error_1, error_2, total_error "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zj6U_rnmJLD0"
   },
   "source": [
    "#### 3. We train our network on training dataset and evaluate them on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lYZv-VAlJKr0",
    "outputId": "3295d303-4a22-488f-d731-5f6f185108a7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EZRA\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\functional.py:1331: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch: 1, itr:   500] total_loss: 132.413657 class_error: 10.403678 error_1: 16.494953 error_2: 11.881919\n",
      "Epoch=1 Test Accuracy=24.81%\n",
      "[epoch: 2, itr:   500] total_loss: 109.972163 class_error: 8.795438 error_1: 13.990342 error_2: 8.027443\n",
      "Epoch=2 Test Accuracy=27.49%\n",
      "[epoch: 3, itr:   500] total_loss: 101.508015 class_error: 8.189035 error_1: 12.608565 error_2: 7.009102\n",
      "Epoch=3 Test Accuracy=33.29%\n",
      "[epoch: 4, itr:   500] total_loss: 93.189327 class_error: 7.496774 error_1: 11.565068 error_2: 6.656518\n",
      "Epoch=4 Test Accuracy=41.58%\n",
      "[epoch: 5, itr:   500] total_loss: 87.466763 class_error: 7.014979 error_1: 10.703561 error_2: 6.613414\n",
      "Epoch=5 Test Accuracy=45.00%\n",
      "[epoch: 6, itr:   500] total_loss: 80.882377 class_error: 6.423967 error_1: 10.065187 error_2: 6.577525\n",
      "Epoch=6 Test Accuracy=49.94%\n",
      "[epoch: 7, itr:   500] total_loss: 74.827049 class_error: 5.882178 error_1: 9.435469 error_2: 6.569800\n",
      "Epoch=7 Test Accuracy=53.95%\n",
      "[epoch: 8, itr:   500] total_loss: 69.574530 class_error: 5.410991 error_1: 8.969353 error_2: 6.495271\n",
      "Epoch=8 Test Accuracy=55.30%\n",
      "[epoch: 9, itr:   500] total_loss: 63.040413 class_error: 4.801278 error_1: 8.607155 error_2: 6.420476\n",
      "Epoch=9 Test Accuracy=56.36%\n",
      "[epoch: 10, itr:   500] total_loss: 58.256591 class_error: 4.370906 error_1: 8.213252 error_2: 6.334281\n",
      "Epoch=10 Test Accuracy=58.26%\n",
      "[epoch: 11, itr:   500] total_loss: 52.607993 class_error: 3.850308 error_1: 7.964290 error_2: 6.140625\n",
      "Epoch=11 Test Accuracy=53.36%\n",
      "[epoch: 12, itr:   500] total_loss: 47.770442 class_error: 3.409003 error_1: 7.680170 error_2: 6.000239\n",
      "Epoch=12 Test Accuracy=59.49%\n",
      "[epoch: 13, itr:   500] total_loss: 40.430809 class_error: 2.711012 error_1: 7.487455 error_2: 5.833229\n",
      "Epoch=13 Test Accuracy=57.71%\n",
      "[epoch: 14, itr:   500] total_loss: 37.329179 class_error: 2.433375 error_1: 7.365142 error_2: 5.630291\n"
     ]
    }
   ],
   "source": [
    "# 1. Initial networks\n",
    "model = MyModel()\n",
    "model = model.cuda()\n",
    "\n",
    "# 3. Store things for visualizing results\n",
    "accuracy_values=[]\n",
    "total_loss_values=[]\n",
    "error1_loss_values=[]\n",
    "error2_loss_values=[]\n",
    "class_loss_values=[]\n",
    "epoch_number=range(n_epochs)\n",
    "\n",
    "\n",
    "\n",
    "# 4. Begin training\n",
    "criterion = LossFunc()\n",
    "# optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum) \n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate) \n",
    "\n",
    "for epoch in epoch_number:\n",
    "    running_loss = 0.0\n",
    "    running_class = 0.0\n",
    "    running_e1 = 0.0\n",
    "    running_e2 = 0.0\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs = inputs.cuda()\n",
    "        labels = labels.cuda()\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        encoded_input, output, prototype_distances, feature_vector_distances, _ = model(inputs)\n",
    "        # X_true not defined\n",
    "        class_error, error_1, error_2, total_error = criterion(output, labels, inputs, feature_vector_distances, prototype_distances)\n",
    "        total_error.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += total_error.item()\n",
    "        running_class += class_error.item()\n",
    "        running_e1 += error_1.item()\n",
    "        running_e2 += error_2.item()\n",
    "    \n",
    "        if i % 500 == 499:    # print every 6000 mini-batches (12500 batches in total)\n",
    "            print('[epoch: %d, itr: %5d] total_loss: %.6f class_error: %.6f error_1: %.6f error_2: %.6f' %\n",
    "                (epoch + 1, i + 1, running_loss/100, running_class/100, running_e1/100, running_e2/100))\n",
    "\n",
    "            total_loss_values.append(running_loss/100)\n",
    "            error1_loss_values.append(running_e1/100)\n",
    "            error2_loss_values.append(running_e2/100)\n",
    "            class_loss_values.append(running_class/100)\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_class = 0.0\n",
    "            running_e1 = 0.0\n",
    "            running_e2 = 0.0\n",
    "\n",
    "\n",
    "    # Evaluate our dataset on test set\n",
    "    correct = 0\n",
    "    total = 0\n",
    "#     model.showWeight()\n",
    "    # Make parameters stay the same without updating\n",
    "    with torch.no_grad():\n",
    "      for images, labels in test_loader:\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "\n",
    "        encoded_input, output, prototype_distances, feature_vector_distances, prototype_vectors = model(images)\n",
    "        # Return the value, index pair of the biggest element. Max function 1 means the biggest in each row, 0 means biggest in each column\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted.cpu()==labels.cpu()).sum().item()\n",
    "        \n",
    "#       model.presentPrototype(prototype_vectors)\n",
    "#       model.presentDecoded(encoded_input)\n",
    "      TestAccuracy = 100 * correct / total;\n",
    "      accuracy_values.append(TestAccuracy) \n",
    "      print('Epoch=%d Test Accuracy=%.2f%%' %\n",
    "                (epoch + 1, TestAccuracy))\n",
    "    \n",
    "    \n",
    "\n",
    "print('Finished Here!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Push each prototype to the closest sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closed_samples = [1000 for i in range(n_prototypes)]\n",
    "closed_distances = [1000 for i in range(n_prototypes)]\n",
    "closed_labels = ['label' for i in range(n_prototypes)]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs = inputs.cuda()\n",
    "        labels = labels.cuda()\n",
    "        if x == 0:\n",
    "            print(labels[0])\n",
    "                \n",
    "        _, _, _, feature_vector_distances, _ = model(inputs)\n",
    "        feature_vector_distances = feature_vector_distances.cpu().tolist()\n",
    "        \n",
    "        # feature_vector_distances is the list of distances from each prototype to every x_i in the latent space torch.Size([10, 100])\n",
    "        for i in range(n_prototypes):\n",
    "            for j in range(len(feature_vector_distances[i])):\n",
    "                if(feature_vector_distances[i][j] < closed_distances[i]):\n",
    "                    closed_distances[i] = feature_vector_distances[i][j]\n",
    "                    closed_samples[i] = inputs[j].cpu()\n",
    "                    closed_labels[i] = labels[j].cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize Prototypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['airplane','bird','car','cat','deer', 'dog','horse','monkey','ship','truck']\n",
    "x = closed_samples\n",
    "print(type(x))\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "for i in range(n_prototypes):\n",
    "    plt.subplot(5,n_prototypes//5,i+1)\n",
    "    img = torchvision.utils.make_grid(x[i])\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.title(\"P{}, Label: {}\".format(i+1, classes[closed_labels[i]]))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QdWEGmHDTNDt"
   },
   "source": [
    "#### 5. Visualize our performance results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 351
    },
    "id": "JHLZETScTNJn",
    "outputId": "3ff5db4c-0e67-406d-bdd6-f896e3a041a4"
   },
   "outputs": [],
   "source": [
    "# First we check values we have so far\n",
    "# print('accuracy_values', accuracy_values)\n",
    "# print('epoch_number', epoch_number)\n",
    "# Plot the data\n",
    "plt.plot(epoch_number, accuracy_values, label='Model')  \n",
    "plt.title('accuracy_values')\n",
    "plt.xlabel('Epoch')#x轴\n",
    "plt.ylabel('Accuracy')#y轴\n",
    "# Add a legend\n",
    "plt.legend(bbox_to_anchor=(1,0.45))\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data\n",
    "plt.plot(epoch_number, total_loss_values, label='Model')  \n",
    "plt.title('total_loss_values')\n",
    "plt.xlabel('Epoch')#x轴\n",
    "plt.ylabel('total_loss')#y轴\n",
    "# Add a legend\n",
    "plt.legend(bbox_to_anchor=(1,0.45))\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data\n",
    "plt.plot(epoch_number, class_loss_values, label='Model')  \n",
    "plt.title('classification_loss_values')\n",
    "plt.xlabel('Epoch')#x轴\n",
    "plt.ylabel('class_loss')#y轴\n",
    "# Add a legend\n",
    "plt.legend(bbox_to_anchor=(1,0.45))\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data\n",
    "plt.plot(epoch_number, error1_loss_values, label='Model')  \n",
    "plt.title('error1_loss_values')\n",
    "plt.xlabel('Epoch')#x轴\n",
    "plt.ylabel('error1_loss')#y轴\n",
    "# Add a legend\n",
    "plt.legend(bbox_to_anchor=(1,0.45))\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data\n",
    "plt.plot(epoch_number, error2_loss_values, label='Model')  \n",
    "plt.title('error2_loss_values')\n",
    "plt.xlabel('Epoch')#x轴\n",
    "plt.ylabel('error2_loss')#y轴\n",
    "# Add a legend\n",
    "plt.legend(bbox_to_anchor=(1,0.45))\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eaLjE4pRvk8-"
   },
   "source": [
    "#### Do more analysis on the final model to see how well it deals with each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PMLUHnhsvhPp",
    "outputId": "7118aea1-758c-4f14-b700-d3f8d59dab3f"
   },
   "outputs": [],
   "source": [
    "classes = ('0', '1', '2', '3', '4', '5', '6', '7', '8', '9')\n",
    "all_class_correct = list(0. for i in range(10))\n",
    "all_class_total = list(0. for i in range(10))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images =images.cuda()\n",
    "        labels =labels.cuda()\n",
    "        _, outputs, _, _, _ = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        c = (predicted == labels)\n",
    "\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            label = labels[i]\n",
    "            all_class_correct[label] += c[i].item() # True is deemed as 1\n",
    "            all_class_total[label] += 1\n",
    "\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    print(\"all class total: \", all_class_total[i],\" all class correct: \", all_class_correct[i])\n",
    "    print('For network: Model Accuracy of %5s : %2d %%' % \n",
    "          (classes[i], 100 * all_class_correct[i] / all_class_total[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rDqy3hvA1EDq"
   },
   "source": [
    "#### Visualize the performance on each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "fxQ5K9Tz1ESG",
    "outputId": "8a605c60-efa8-4371-827f-d1dfccb5804d"
   },
   "outputs": [],
   "source": [
    "classes = ['plane','car','bird','cat','deer', 'dog','frog','horse','ship','truck']\n",
    "\n",
    "for i, x in enumerate(all_class_correct):\n",
    "    all_class_correct[i] = 100 * all_class_correct[i] / all_class_total[i]\n",
    "print('all_class_correct', all_class_correct)\n",
    "\n",
    "\n",
    "plt.bar(classes, all_class_correct, color=['r','y','b','m','coral','g','c','k','dodgerblue','orange'])\n",
    "plt.title('Model performance on each class')\n",
    "plt.xlabel('Class Name')#x轴\n",
    "plt.ylabel('Percentage of Correctness in %')#y轴\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
